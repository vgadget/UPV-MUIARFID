{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b79a5b",
   "metadata": {},
   "source": [
    "# Traffic Sign Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8088d",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f39c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import models\n",
    "\n",
    "#MANAGEMENT PURPOSES ONLY\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd437c",
   "metadata": {},
   "source": [
    "## Data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a521f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"./DATASET/\"\n",
    "img_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16cc3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    images = []\n",
    "    classes = []    \n",
    "    rows = pd.read_csv(dataset)\n",
    "    rows = rows.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "    with tqdm(total=len(rows)+1) as pbar:\n",
    "    \n",
    "        for i, row in rows.iterrows():\n",
    "            img_class = row[\"ClassId\"]\n",
    "            img_path = row[\"Path\"]        \n",
    "            image = os.path.join(data_path, img_path)\n",
    "\n",
    "            image = cv2.imread(image)\n",
    "            image_rs = cv2.resize(image, (img_size, img_size), 3)        \n",
    "\n",
    "            R, G, B = cv2.split(image_rs)     \n",
    "\n",
    "            img_r = cv2.equalizeHist(R)\n",
    "            img_g = cv2.equalizeHist(G)\n",
    "            img_b = cv2.equalizeHist(B)        \n",
    "\n",
    "            new_image = cv2.merge((img_r, img_g, img_b))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "            images.append(new_image)\n",
    "            classes.append(img_class)\n",
    "            \n",
    "        pbar.container.children[-2].style.bar_color = '#00FF00' # Set bar to green color at end\n",
    "    \n",
    "    X = np.array(images)\n",
    "    y = np.array(classes)\n",
    "    \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ac8d4",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86392be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5951857dcb9c4aecac38cffa5f639adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705d5f39cf674e35b363e50b9a72c97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_path = data_path + \"Train.csv\"\n",
    "test_data_path = data_path + \"Test.csv\"\n",
    "\n",
    "\n",
    "(Xtrain, Ytrain) = load_data(train_data_path)\n",
    "(Xtest, Ytest) = load_data(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb8930",
   "metadata": {},
   "source": [
    "### Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d62fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = Xtrain.astype(\"float32\") / 255.0\n",
    "Xtest = Xtest.astype(\"float32\") / 255.0\n",
    "\n",
    "num_labels = len(np.unique(Ytrain))\n",
    "Ytrain = to_categorical(Ytrain, num_labels)\n",
    "Ytest = to_categorical(Ytest, num_labels)\n",
    "\n",
    "class_totals = Ytrain.sum(axis=0)\n",
    "class_weight = class_totals.max() / class_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a63a9",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4627929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a821755",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11704953",
   "metadata": {},
   "source": [
    " Data augmentation creates modified versions of the images in our dataset. It allows us to add images to our dataset without us having to collect new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f782a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range = 10,\n",
    "    zoom_range = 0.15,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range = 0.15,\n",
    "    horizontal_flip = False,\n",
    "    vertical_flip = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b300bf",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061749ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSignClassifier:\n",
    "    \n",
    "    def createCNN(width, height, depth, classes):\n",
    "        \n",
    "        \"\"\"\n",
    "         We will be using the Sequential API, \n",
    "         which allows us to create the model layer-by-layer\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "         First convolutional layer. Define output dim(8 X 8 X 3)\n",
    "         Activation function “relu”\n",
    "        \"\"\"\n",
    "        model.add(Conv2D(8, (5, 5), input_shape=inputShape, activation=\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        \"\"\"\n",
    "        This time we include batch normalization. It just speeds up training.\n",
    "        \"\"\"\n",
    "        model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        \"\"\"\n",
    "        The output in the final dense layer is equal to the number of classes that we have.\n",
    "        \"\"\"\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation=\"relu\"))        \n",
    "        \n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783c36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RoadSignClassifier.createCNN(\n",
    "    width = img_size, \n",
    "    height = img_size, \n",
    "    depth = 3, \n",
    "    classes = len(class_totals)\n",
    ")\n",
    "\n",
    "optimizer = Adam(\n",
    "    lr = learning_rate, \n",
    "    decay = (learning_rate / epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "196a890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = optimizer, \n",
    "    loss = \"categorical_crossentropy\", \n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5056719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "613/613 [==============================] - 25s 41ms/step - loss: 4.3298 - accuracy: 0.4898 - val_loss: 0.9235 - val_accuracy: 0.7085\n",
      "Epoch 2/20\n",
      "613/613 [==============================] - 25s 41ms/step - loss: 1.8169 - accuracy: 0.7510 - val_loss: 0.4750 - val_accuracy: 0.8572\n",
      "Epoch 3/20\n",
      "613/613 [==============================] - 26s 42ms/step - loss: 1.2283 - accuracy: 0.8265 - val_loss: 0.3666 - val_accuracy: 0.8899\n",
      "Epoch 4/20\n",
      "613/613 [==============================] - 26s 42ms/step - loss: 0.8814 - accuracy: 0.8730 - val_loss: 0.5338 - val_accuracy: 0.8416\n",
      "Epoch 5/20\n",
      "613/613 [==============================] - 29s 47ms/step - loss: 0.7104 - accuracy: 0.8923 - val_loss: 0.3374 - val_accuracy: 0.9035\n",
      "Epoch 6/20\n",
      "613/613 [==============================] - 28s 46ms/step - loss: 0.6225 - accuracy: 0.9084 - val_loss: 0.3496 - val_accuracy: 0.9018\n",
      "Epoch 7/20\n",
      "613/613 [==============================] - 28s 45ms/step - loss: 0.5776 - accuracy: 0.9124 - val_loss: 0.3401 - val_accuracy: 0.9061\n",
      "Epoch 8/20\n",
      "613/613 [==============================] - 28s 46ms/step - loss: 0.4968 - accuracy: 0.9250 - val_loss: 0.2586 - val_accuracy: 0.9327\n",
      "Epoch 9/20\n",
      "613/613 [==============================] - 28s 46ms/step - loss: 0.4460 - accuracy: 0.9315 - val_loss: 0.2969 - val_accuracy: 0.9195\n",
      "Epoch 10/20\n",
      "613/613 [==============================] - 28s 46ms/step - loss: 0.4063 - accuracy: 0.9379 - val_loss: 0.2840 - val_accuracy: 0.9310\n",
      "Epoch 11/20\n",
      "613/613 [==============================] - 29s 47ms/step - loss: 0.3947 - accuracy: 0.9405 - val_loss: 0.4029 - val_accuracy: 0.8914: 4s - loss: - ETA: 1s - los\n",
      "Epoch 12/20\n",
      "613/613 [==============================] - 29s 48ms/step - loss: 0.3522 - accuracy: 0.9454 - val_loss: 0.1944 - val_accuracy: 0.9464\n",
      "Epoch 13/20\n",
      "613/613 [==============================] - 30s 49ms/step - loss: 0.3135 - accuracy: 0.9502 - val_loss: 0.2173 - val_accuracy: 0.9457\n",
      "Epoch 14/20\n",
      "613/613 [==============================] - 29s 47ms/step - loss: 0.3128 - accuracy: 0.9500 - val_loss: 0.1973 - val_accuracy: 0.9444\n",
      "Epoch 15/20\n",
      "613/613 [==============================] - 32s 52ms/step - loss: 0.3187 - accuracy: 0.9538 - val_loss: 0.2185 - val_accuracy: 0.9407\n",
      "Epoch 16/20\n",
      "613/613 [==============================] - 30s 50ms/step - loss: 0.2728 - accuracy: 0.9580 - val_loss: 0.2277 - val_accuracy: 0.9431\n",
      "Epoch 17/20\n",
      "613/613 [==============================] - 30s 49ms/step - loss: 0.2661 - accuracy: 0.9576 - val_loss: 0.1919 - val_accuracy: 0.9504\n",
      "Epoch 18/20\n",
      "613/613 [==============================] - 28s 46ms/step - loss: 0.2335 - accuracy: 0.9622 - val_loss: 0.2031 - val_accuracy: 0.9481\n",
      "Epoch 19/20\n",
      "613/613 [==============================] - 27s 44ms/step - loss: 0.2405 - accuracy: 0.9617 - val_loss: 0.1870 - val_accuracy: 0.9500\n",
      "Epoch 20/20\n",
      "613/613 [==============================] - 28s 46ms/step - loss: 0.2237 - accuracy: 0.9647 - val_loss: 0.2241 - val_accuracy: 0.9430\n"
     ]
    }
   ],
   "source": [
    "fit = model.fit(\n",
    "    \n",
    "    data_augmentation.flow(\n",
    "        Xtrain, \n",
    "        Ytrain, \n",
    "        batch_size = batch_size\n",
    "    ), \n",
    "    epochs = epochs,\n",
    "    \n",
    "    validation_data = (Xtest, Ytest),\n",
    "    \n",
    "    class_weight = dict(enumerate(class_weight.flatten(), 0)),\n",
    "    \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "832753ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TSR_Model_96per\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "575a9369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MODELS/TSR_Model_96per\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f'./MODELS/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea44eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(f'./MODELS/{model_name}')\n",
    "model.compile(\n",
    "    optimizer = optimizer, \n",
    "    loss = \"categorical_crossentropy\", \n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
